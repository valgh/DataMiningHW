# -*- coding: utf-8 -*-
"""BERTTextClass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VIg_eMDRC7rFSUQFHgREif3b5J4TKMCt
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)


import os
#if you wish to run this locally, uncomment this code!
#from os.path import dirname
#curr = os.getcwd()
#directory = dirname(curr)
#print(directory)
#with open(directory+"/content/lyrics.csv") as f:
with open("/content/lyrics.csv") as f: #comment out this line if you are running this locally 
  df = pd.read_csv(f, names=['index', 'song', 'year', 'artist', 'genre', 'lyrics'], dtype={'lyrics': str})
  #some preprocessing steps on lyrics: lower their case, and remove useless words/symbols which recur quite often
  df['lyrics'] = df['lyrics'].str.lower()
  df['lyrics'] = df['lyrics'].str.strip('[]')
  df['lyrics'] = df['lyrics'].str.strip('()')
  df["lyrics"] = df['lyrics'].str.replace('chorus','')
  df["lyrics"] = df['lyrics'].str.replace('verse','')
  df["lyrics"] = df['lyrics'].str.replace('intro','')
  lyrics = df['lyrics'].to_dict()
  genres = df['genre'].to_dict()
            
#now we only consider 'valid' lyrics, that is to say, only those lyrics that have actually
#more than 10 words and are NOT empty (there are empty lyrics in the dataset)
valid_lyrics = {}
valid_genres = {}
for key in lyrics:
    if len(str(lyrics[key]))>10:
        valid_lyrics[key] = lyrics[key]
        valid_genres[key] = genres[key]

print("\nRemaining valid lyrics and corresponding genres:\n")
print(len(valid_lyrics))
print(len(valid_genres))

#no ned for further preprocessing step here, Bert Tokenizer will handle this 
#in the next step

#these are all the genres we have
genres_final = {}
for g in valid_genres:
    if valid_genres[g] not in genres_final:
        genres_final[valid_genres[g]] = [valid_genres[g]]
print("\nClasses in original dataset:\n")
for key in genres_final:
  print(key)

#we will not consider genres such as "Other" or "Not available", we consider them
#as unclassified songs
#we only deal with these 5 classes
genre2lyrics = {}
genre2lyrics["Rock"] = []
genre2lyrics["Hip-Hop"] = []
genre2lyrics["Electronic"] = []
genre2lyrics["Jazz"] = []
genre2lyrics["Country"] = []
for key in valid_lyrics:
    if valid_genres[key] in genre2lyrics:
        genre2lyrics[valid_genres[key]].append(valid_lyrics[key])
print("\nClasses we are going to consider in the new dataset:\n")
for key in genre2lyrics:
    print(key, len(genre2lyrics[key]))
    
#to make our dataset more balanced, we arbitrarily select
#only 1500 lyrics from each of the five genres, and work with them.
genre2lyrics["Rock"] = genre2lyrics["Rock"][:1500]
genre2lyrics["Hip-Hop"] = genre2lyrics["Hip-Hop"][:1500]
genre2lyrics["Electronic"] = genre2lyrics["Electronic"][:1500]
genre2lyrics["Jazz"] = genre2lyrics["Jazz"][:1500]
genre2lyrics["Country"] = genre2lyrics["Country"][:1500]
texts = []
labels = []
for genre in genre2lyrics:
  for lyric in genre2lyrics[genre]:
    texts.append(lyric)
    labels.append(genre)
#now we split it into train, development and test
from sklearn.model_selection import train_test_split
rest_texts, test_texts, rest_labels, test_labels = train_test_split(texts, labels, test_size=0.1, random_state=1)
train_texts, dev_texts, train_labels, dev_labels = train_test_split(rest_texts, rest_labels, test_size=0.1, random_state=1)

print("Train size:", len(train_texts))
print("Dev size:", len(dev_texts))
print("Test size:", len(test_texts))
#and now we get the number of labels with corresponding indexes
target_names = list(set(labels))
label2idx = {label: idx for idx, label in enumerate(target_names)}
print(label2idx)
print(type(train_texts))
print(type(train_labels))

#END OF PREPROCESSING STEP: to speed things up if needed for application purposes, one might 
#save the obtained reduced dataset of interest in a .tsv file

!pip install transformers

import torch
import transformers
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BERT_MODEL = "bert-base-uncased"

#initialize the BertTokenizer from the base-uncased model

from transformers.tokenization_bert import BertTokenizer

tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)

#initialize the Bert model for the classification
from transformers.modeling_bert import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels = len(label2idx))
model.to(device)

#we now create a BertInputItem class
MAX_SEQ_LENGTH=100

class BertInputItem(object):
  def __init__(self, text, input_ids, input_mask, segment_ids, label_id):
    self.text = text
    self.input_ids = input_ids
    self.input_mask = input_mask
    self.segment_ids = segment_ids
    self.label_id = label_id

def convert_examples_to_inputs(example_texts, example_labels, label2idx, max_seq_length, tokenizer, verbose=0):
  input_items = []
  examples = zip(example_texts, example_labels)
  for (ex_index, (text,label)) in enumerate(examples):
    #create a list of token ids
     input_ids = tokenizer.encode(f"[CLS] {text} [SEP]")
     if len(input_ids) > max_seq_length:
      input_ids = input_ids[:max_seq_length]

      # All our tokens are in the first input segment (id 0).
      segment_ids = [0] * len(input_ids)

      # The mask has 1 for real tokens and 0 for padding tokens. Only real
      # tokens are attended to.
      input_mask = [1] * len(input_ids)

      # Zero-pad up to the sequence length.
      padding = [0] * (max_seq_length - len(input_ids))
      input_ids += padding
      input_mask += padding
      segment_ids += padding

      assert len(input_ids) == max_seq_length
      assert len(input_mask) == max_seq_length
      assert len(segment_ids) == max_seq_length

      label_id = label2idx[label]

      input_items.append(
          BertInputItem(text=text,
                        input_ids=input_ids,
                        input_mask=input_mask,
                        segment_ids=segment_ids,
                        label_id=label_id))

        
  return input_items

train_features = convert_examples_to_inputs(train_texts, train_labels, label2idx, MAX_SEQ_LENGTH, tokenizer, verbose=0)
dev_features = convert_examples_to_inputs(dev_texts, dev_labels, label2idx, MAX_SEQ_LENGTH, tokenizer)
test_features = convert_examples_to_inputs(test_texts, test_labels, label2idx, MAX_SEQ_LENGTH, tokenizer)

#now, we initialize a data loader
from torch.utils.data import TensorDataset, DataLoader, SequentialSampler

def get_data_loader(features, max_seq_length, batch_size, shuffle=True): 

    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)
    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)
    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)
    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)
    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)

    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)
    return dataloader

BATCH_SIZE = 50

train_dataloader = get_data_loader(train_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=True)
dev_dataloader = get_data_loader(dev_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)
test_dataloader = get_data_loader(test_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)

#END OF BUILDING THE PYTORCH DATASET

#function to evaluate the BERT nn
def evaluate(model, dataloader):
    model.eval()
    
    eval_loss = 0
    nb_eval_steps = 0
    predicted_labels, correct_labels = [], []

    for step, batch in enumerate(tqdm(dataloader, desc="Evaluation iteration")):
        batch = tuple(t.to(device) for t in batch)
        input_ids, input_mask, segment_ids, label_ids = batch

        with torch.no_grad():
            tmp_eval_loss, logits = model(input_ids, attention_mask=input_mask,
                                          token_type_ids=segment_ids, labels=label_ids)

        outputs = np.argmax(logits.to('cpu'), axis=1)
        label_ids = label_ids.to('cpu').numpy()
        
        predicted_labels += list(outputs)
        correct_labels += list(label_ids)
        
        eval_loss += tmp_eval_loss.mean().item()
        nb_eval_steps += 1

    eval_loss = eval_loss / nb_eval_steps
    
    correct_labels = np.array(correct_labels)
    predicted_labels = np.array(predicted_labels)
        
    return eval_loss, correct_labels, predicted_labels

#parameters to tune the training step
from transformers.optimization import AdamW, get_linear_schedule_with_warmup

GRADIENT_ACCUMULATION_STEPS = 1
NUM_TRAIN_EPOCHS = 20
LEARNING_RATE = 5e-5
WARMUP_PROPORTION = 0.1
MAX_GRAD_NORM = 5

num_train_steps = int(len(train_dataloader.dataset) / BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS * NUM_TRAIN_EPOCHS)
num_warmup_steps = int(WARMUP_PROPORTION * num_train_steps)

param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
    ]

optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, correct_bias=False)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)

#train the network
import torch
import os
from tqdm import trange
from tqdm import tqdm_notebook as tqdm
from sklearn.metrics import classification_report, precision_recall_fscore_support

OUTPUT_DIR = "/content/"
MODEL_FILE_NAME = "pytorch_model.bin"
PATIENCE = 2

loss_history = []
no_improvement = 0
for _ in trange(int(NUM_TRAIN_EPOCHS), desc="Epoch"):
    model.train()
    tr_loss = 0
    nb_tr_examples, nb_tr_steps = 0, 0
    for step, batch in enumerate(tqdm(train_dataloader, desc="Training iteration")):
        batch = tuple(t.to(device) for t in batch)
        input_ids, input_mask, segment_ids, label_ids = batch

        outputs = model(input_ids, attention_mask=input_mask, token_type_ids=segment_ids, labels=label_ids)
        loss = outputs[0]

        if GRADIENT_ACCUMULATION_STEPS > 1:
            loss = loss / GRADIENT_ACCUMULATION_STEPS

        loss.backward()
        tr_loss += loss.item()

        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)  
            
            optimizer.step()
            optimizer.zero_grad()
            scheduler.step()
            
    dev_loss, _, _ = evaluate(model, dev_dataloader)
    
    print("Loss history:", loss_history)
    print("Dev loss:", dev_loss)
    
    if len(loss_history) == 0 or dev_loss < min(loss_history):
        no_improvement = 0
        model_to_save = model.module if hasattr(model, 'module') else model
        output_model_file = os.path.join(OUTPUT_DIR, MODEL_FILE_NAME)
        torch.save(model_to_save.state_dict(), output_model_file)
    else:
        no_improvement += 1
    
    if no_improvement >= PATIENCE: 
        print("No improvement on development set. Finish training.")
        break
        
    
    loss_history.append(dev_loss)

#EVALUATION OF THE NETWORK

#evaluate the predictions
model_state_dict = torch.load(os.path.join(OUTPUT_DIR, MODEL_FILE_NAME), map_location=lambda storage, loc: storage)
model = BertForSequenceClassification.from_pretrained(BERT_MODEL, state_dict=model_state_dict, num_labels = len(target_names))
model.to(device)

model.eval()

_, train_correct, train_predicted = evaluate(model, train_dataloader)
_, dev_correct, dev_predicted = evaluate(model, dev_dataloader)
_, test_correct, test_predicted = evaluate(model, test_dataloader)

from sklearn.metrics import confusion_matrix
print(confusion_matrix(test_correct, test_predicted))

print("Training performance:", precision_recall_fscore_support(train_correct, train_predicted, average="micro"))
print("Development performance:", precision_recall_fscore_support(dev_correct, dev_predicted, average="micro"))
print("Test performance:", precision_recall_fscore_support(test_correct, test_predicted, average="micro"))

bert_accuracy = np.mean(test_predicted == test_correct)
print(bert_accuracy)

print(classification_report(test_correct, test_predicted, target_names=target_names))