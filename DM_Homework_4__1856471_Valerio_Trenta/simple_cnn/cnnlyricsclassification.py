# -*- coding: utf-8 -*-
"""CNNLyricsClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Mpetnbgxp1BMdtFjB-ASzuYL43pYe7k
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import os
#if you wish to run this locally, uncomment this code!
#from os.path import dirname
#curr = os.getcwd()
#directory = dirname(curr)
#print(directory)
#with open(directory+"/content/lyrics.csv") as f:
with open("/content/lyrics.csv") as f: #comment out this line if you are running this locally 
  df = pd.read_csv(f, names=['index', 'song', 'year', 'artist', 'genre', 'lyrics'], dtype={'lyrics': str})
  #some preprocessing steps on lyrics: lower their case, and remove useless words/symbols which recur quite often
  df['lyrics'] = df['lyrics'].str.lower()
  df['lyrics'] = df['lyrics'].str.strip('[]')
  df['lyrics'] = df['lyrics'].str.strip('()')
  df["lyrics"] = df['lyrics'].str.replace('chorus','')
  df["lyrics"] = df['lyrics'].str.replace('verse','')
  df["lyrics"] = df['lyrics'].str.replace('intro','')
  lyrics = df['lyrics'].to_dict()
  genres = df['genre'].to_dict()
            
#now we only consider 'valid' lyrics, that is to say, only those lyrics that have actually
#more than 10 words and are NOT empty (there are empty lyrics in the dataset)
valid_lyrics = {}
valid_genres = {}
for key in lyrics:
    if len(str(lyrics[key]))>10:
        valid_lyrics[key] = lyrics[key]
        valid_genres[key] = genres[key]

print("\nRemaining valid lyrics and corresponding genres:\n")

print(len(valid_lyrics))
print(len(valid_genres))

#now a preprocessing step to tokenize, remove punctuation and stopwords from each lyric
preprocessed_lyrics = {}
import nltk
nltk.download('stopwords')
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
tokenizer = RegexpTokenizer(r'\w+')
stop_words = set(stopwords.words('english'))
for key in valid_lyrics: #for each lyric, tokenize it, remove punctuation and remove stopwords
    tokenized_lyric = tokenizer.tokenize(str(valid_lyrics[key]))
    preprocessed_lyrics[key] = []
    for word in tokenized_lyric:
        if word not in stop_words:
            if word not in preprocessed_lyrics[key]:
                preprocessed_lyrics[key].append(word)
#we now have for each lyric, a bag of words for the lyric

#these are all the genres we have
genres_final = {}
for g in valid_genres:
    if valid_genres[g] not in genres_final:
        genres_final[valid_genres[g]] = [valid_genres[g]]
print("\nGenres in original dataset:\n")
for key in genres_final:
  print(key)

#we will not consider genres such as "Other" or "Not available", we consider them
#as unclassified songs
#we only deal with these 5 classes
genre2lyrics = {}
genre2lyrics["Rock"] = []
genre2lyrics["Hip-Hop"] = []
genre2lyrics["Electronic"] = []
genre2lyrics["Jazz"] = []
genre2lyrics["Country"] = []
for key in preprocessed_lyrics:
    if valid_genres[key] in genre2lyrics:
        genre2lyrics[valid_genres[key]].append(preprocessed_lyrics[key])
print("\nGenres in our dataset:\n")        
for key in genre2lyrics:
    print(key, len(genre2lyrics[key]))

#notice that we have far more lyrics/songs for Rock class and Hip Hop class with respect
#to the other classes, so we need to balance the dataset and we can do this by
#selecting a certain number of songs from each class, same number for each of them

#to make our dataset more balanced, we arbitrarily select
#only 1500 lyrics from each of the five genres, and work with them.
genre2lyrics["Rock"] = genre2lyrics["Rock"][:1500]
genre2lyrics["Hip-Hop"] = genre2lyrics["Hip-Hop"][:1500]
genre2lyrics["Electronic"] = genre2lyrics["Electronic"][:1500]
genre2lyrics["Jazz"] = genre2lyrics["Jazz"][:1500]
genre2lyrics["Country"] = genre2lyrics["Country"][:1500]
texts = []
labels = []
for genre in genre2lyrics:
  for lyric in genre2lyrics[genre]:
    texts.append(lyric)
    labels.append(genre)
#now we split it into train, development and test
from sklearn.model_selection import train_test_split
rest_texts, test_texts, rest_labels, test_labels = train_test_split(texts, labels, test_size=0.1, random_state=1)
train_texts, dev_texts, train_labels, dev_labels = train_test_split(rest_texts, rest_labels, test_size=0.1, random_state=1)

print("Train size:", len(train_texts))
print("Dev size:", len(dev_texts)) #not actually used here
print("Test size:", len(test_texts))
#and now we get the number of labels with corresponding indexes
target_names = list(set(labels))
print(target_names)
label2idx = {label: idx for idx, label in enumerate(target_names)}
print(label2idx)
print(type(train_texts))
print(type(train_labels))

#END OF PREPROCESSING STEP: to speed things up if needed for application purposes, one might 
#save the obtained reduced dataset of interest in a .tsv file

#we now need to create a vocabulary, meaning we need to encode
#each of the words in our corpus. We will encode the words as follows: 
#each word is a vector [x,x,x,x] with x = 1 if word is in a lyric from genre x, 0 otherwise.
to_train = train_texts + dev_texts
to_label = train_labels + dev_labels
def create_vocabulary(train_texts):
  vocab = {}
  for lyric in train_texts:
    for word in lyric:
      if word not in vocab:
        vocab[word] = np.zeros(len(label2idx), dtype=int)
  return vocab

vocab = create_vocabulary(to_train)
print(len(vocab))
print(vocab["rock"])

#now we need to update the vocabulary.
def fill_vectors(vocabulary, train_texts, train_labels):
  for word in vocabulary:
    for lyric in train_texts:
      if word in lyric:
        idx = train_texts.index(lyric)
        if train_labels[idx] == "Hip-Hop":
          vocabulary[word][label2idx["Hip-Hop"]] = 1
        elif train_labels[idx] == "Electronic":
          vocabulary[word][label2idx["Electronic"]] = 1
        elif train_labels[idx] == "Jazz":
          vocabulary[word][label2idx["Jazz"]] = 1
        elif train_labels[idx] == "Country":
          vocabulary[word][label2idx["Country"]] = 1
        else:
          vocabulary[word][label2idx["Rock"]] = 1 #it's rock
  return vocabulary

vocab_filled = fill_vectors(vocab, to_train, to_label)
print(len(vocab_filled))
print(vocab_filled["rock"])

#now we want to load this embedding in an embedded layer in the neural network
#so, we need to build a matrix of weights of shape (n_words_in_vocab, vectors_size)
n_words_in_vocab = len(vocab_filled)
vectors_size = len(label2idx)
words_found = 0

matrix = np.zeros((n_words_in_vocab, vectors_size))
index = -1
for keyword in vocab_filled.keys():
    index+=1
    try:
        matrix[index] = vocab_filled[keyword]
        words_found+=1
    except:
        matrix[index] = np.random.normal(scale=0.6, size=(vectors_size, ))
#did we find all the words?        
print(words_found)
#for example:
print(matrix[0])

#now that we have our vocabulary, next point is to encode our training and test datasets
#to create a dataset which can be exploited via Pytorch
def encode_lyrics(dataset):
  lyrics = []
  vocab = list(vocab_filled.keys())
  for lyric in dataset:
    ly = []
    for word in lyric:
      try:
        indx = vocab.index(word)
      except:
        indx = 0
      ly.append(indx)
    lyrics.append(ly)
  return lyrics


def encode_labels(dataset):
  labels = []
  for genre in dataset:
    labels.append(label2idx[genre])
  return np.asarray(labels)

y_training = encode_labels(to_label)
print(y_training[1])
X_training = encode_lyrics(to_train)
print(X_training[1])

#lyrics have different length, so we decide to either pad the shorter ones or cut the longer ones
#to a prefixed length of 100 words
def padding(dataset, l):
  for lyric in dataset:
    if len(lyric) < l:
      while len(lyric)!=l:
        lyric.append(0)
    else:
      while len(lyric) > l:
        lyric.pop()
  return dataset

X_training = padding(X_training, 100)
for ly in X_training:
  if len(ly) > 100:
    print(len(ly))
print(y_training[1])
print(X_training[1])

#now we can create the dataset and the dataloader for Pytorch
import torch
from torch.utils.data import TensorDataset, DataLoader

#Tensor Dataset
train_data = TensorDataset(torch.LongTensor(X_training), torch.from_numpy(y_training))
print(train_data)
print(train_data[1])
#DataLoader
train_loader = DataLoader(train_data, shuffle=True, batch_size=50)
print(train_loader)

#END OF BUILDING THE PYTORCH DATASET

#now it's time to build the neural network
import torch
from torch import nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.filters = 100
        self.output = len(label2idx)
        #embedding layer for the nn
        self.n_words_in_vocab = len(vocab_filled)
        self.vectors_size = len(label2idx)
        self.embedding = nn.Embedding(self.n_words_in_vocab, self.vectors_size)
        self.embedding.weight = nn.Parameter(torch.from_numpy(matrix), requires_grad = False)
        #freeze the embeddings, they will only be a lookup table
        #convolutional layer
        self.conv = nn.Conv2d(1, self.filters, self.vectors_size)
        #FULLY CONNECTED LAYER:
        self.fc1 = nn.Linear(self.filters, self.output) #desired number of output layers = 5, it's the number of genres we have!
        self.softmax = nn.Softmax(1)

    def forward(self, x):
        x = self.embedding(x)
        x = x.unsqueeze(1) #to have an input channel dimension=1 that is the one that conv layer expects,
        #so we are basically making a 1-dimensional cross-correlation
        x = F.relu(self.conv(x)).squeeze(3) #pooling will expect a 4-dimensional input
        x = F.max_pool1d(x, x.size(2))  
        x = x.view(x.shape[0], -1) #flatten the output of pooling to provide the final fc layer the expected input
        #apply dropout ONLY IF NOT TRAINING
        x = F.dropout(x, training = self.training)
        x = self.fc1(x)
        return self.softmax(x)

#enabling GPU
use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")
from torch import nn, optim
net = Net()

net.to(device)

loss = nn.CrossEntropyLoss()
opt = optim.Adam(params=net.parameters(), lr = 0.001)

print(net)

net = net.float()

#training the CNN
def train_step(x, y):
  net.train()
  y_pred = net(x)
  loss_epoch = loss(y_pred, y)
  loss_epoch.backward()
  opt.step()
  opt.zero_grad()

#training step 
import torch
from tqdm import tqdm_notebook as tqdm
for epoch in tqdm(range(20)):
  net.train()
  for Xb, yb in train_loader:
    Xb = Xb.to(device) #move into device 
    yb = yb.to(device) #move into device 
    train_step(Xb, yb)

#now building the test_dataset
y_testing = encode_labels(test_labels)
X_test = encode_lyrics(test_texts)
X_test = padding(X_test, 100)
print(y_testing[1])
print(X_test[1])

#Tensor Dataset
test_data = TensorDataset(torch.LongTensor(X_test), torch.from_numpy(y_testing))
print(test_data)
print(test_data[1])
#DataLoader
test_loader = DataLoader(test_data, shuffle=True, batch_size=50)
print(test_loader)

#EVALUATION OF THE NETWORK

#now evaluating the performances on the test_dataset
with torch.no_grad():
  Y_pred = []
  Y_true = []
  net.eval()
  for Xb, yb in test_loader:
    Xb = Xb.to(device)
    y_pred = net(Xb)
    yb = yb.to(device)
    correct = (y_pred.max(dim=1)[1] == yb)
    Y_pred.append(y_pred.max(dim=1)[1].cpu())
    Y_true.append(yb.cpu())
  print(torch.mean(correct.float()).item())

from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, classification_report
print(confusion_matrix(torch.cat(Y_true), torch.cat(Y_pred)))

print("\nTest performance:", precision_recall_fscore_support(torch.cat(Y_true), torch.cat(Y_pred), average="micro"))


print(classification_report(torch.cat(Y_true), torch.cat(Y_pred), target_names=target_names))